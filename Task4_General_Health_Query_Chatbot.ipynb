{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4: General Health Query Chatbot (Prompt Engineering)"
      ],
      "metadata": {
        "id": "1wFrplshXfyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Goal:\n",
        "Create a chatbot that can answer general health-related questions using an LLM (Large\n",
        "Language Model)"
      ],
      "metadata": {
        "id": "IZpbEx86jw7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Objective:\n",
        "We'll use DistilGPT2 from Hugging Face to build a friendly chatbot without needing an API key."
      ],
      "metadata": {
        "id": "lTthE5xsZNnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nxXYH8ukZV8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers accelerate torch\n"
      ],
      "metadata": {
        "id": "TlSnBQswbc5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  signup/login to hugging face account\n",
        "Go to https://huggingface.co/settings/tokens i.e settings -> access tohen -> create new token\n",
        "\n",
        "Click “New token”\n",
        "\n",
        "Name: colab_access\n",
        "\n",
        "Role: Read\n",
        "\n",
        "Copy the token (looks like: hf_abc123xyz...)\n",
        "\n",
        "In your Colab notebook, run:"
      ],
      "metadata": {
        "id": "6P0iCsCT_XPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# after getting the token from huggingface hub account , paste that token when asked\n",
        "\n",
        "# !huggingface-cli login\n"
      ],
      "metadata": {
        "id": "nUsaXDCeDAiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# LLOad free gpt model\n",
        "\n",
        "model_name=\"distilgpt2\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_name) # helpful for encoding input text i.e tokenization\n",
        "model=AutoModelForCausalLM.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "6ks79MJSh7a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de287628-0a66-4202-bd2b-3cc4ec53785d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# move model to cpu or gpu but in my system GPU is not installed\n",
        "\n",
        "device= torch.device('cpu' )\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dpq1P-nZh7hL",
        "outputId": "05810a3f-c471-4919-ab12-8b05d78a5a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create chatbot function"
      ],
      "metadata": {
        "id": "2Ix1Dkph6m0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## create the cahtbot function\n",
        "\n",
        "def chatbot_response(user_input):\n",
        "    prompt = (\n",
        "        \"You are a professional and friendly medical assistant. \"\n",
        "        \"Always provide short, clear, and helpful answers to common health questions. \"\n",
        "        f\"User: {user_input}\\nAssistant:\" ) # it tells the GPT to act as an medical assistant only.\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    output = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=100,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the bot's answer\n",
        "    if \"Assistant:\" in response:\n",
        "        return response.split(\"Assistant:\")[-1].strip()\n",
        "    else:\n",
        "        return response.strip()\n",
        "\n"
      ],
      "metadata": {
        "id": "7fwgadaxlZHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in ['exit', 'quit']:\n",
        "        print(\"Goodbye! \")\n",
        "        break\n",
        "    reply = chatbot_response(user_input)\n",
        "    print(\"Bot:\", reply)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VlffEoo8LZN",
        "outputId": "c08d803a-c5e1-4c7f-cd8f-4bfdb008eec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: what causes fever?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: fever. When you can't get it, it's not contagious. It's not contagious. It's not contagious. It's not contagious. It's not contagious. It's not contagious. It's not contagious. It's not contagious. It's not contagious.\n",
            "The first time you see a doctor in a hospital you know you can get fever. You can't get it. You can't get it. You can't get it. You can't get it.\n",
            "If you get\n",
            "You: what is flu ?\n",
            "Bot: flu is a common flu virus.\n",
            "Consequences: flu is a common flu virus. Symptoms of flu include fever, headache, nausea, vomiting, and vomiting. Symptoms of flu include fever, headache, nausea, vomiting, and vomiting. Symptoms of flu include fever, headache, nausea, vomiting, and vomiting. Symptoms of flu include fever, headache, nausea, vomiting, and vomiting. Symptoms of flu include fever, headache, nausea, vomiting, and vomiting. Symptoms of flu include fever,\n",
            "You: how to avoid headach?\n",
            "Bot: how can you help your doctor or physician know what is causing your headaches?\n",
            "How can you help your doctor or physician know what is causing your headaches?\n",
            "Ask: how do you get rid\n",
            "You: exit\n",
            "Goodbye! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sLWWKv0T83DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note:\n",
        "### This chatbot may give you short and not precise answers because it is free and lightweight."
      ],
      "metadata": {
        "id": "vqNPHEqLP3m5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model is small and fast, but responses may be short or generic.\n",
        "\n",
        "It’s good for practicing basic prompt engineering.\n",
        "\n",
        "It's 100% free and runs offline."
      ],
      "metadata": {
        "id": "Ik9cEF7O83S_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vqr0f_qo84Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions you can ask from it.\n",
        " General Health & Symptoms\n",
        "What causes a sore throat?\n",
        "\n",
        "Why do people get fever?\n",
        "\n",
        "What are common cold symptoms?\n",
        "\n",
        "What is the flu?\n",
        "\n",
        "How can I avoid getting sick?"
      ],
      "metadata": {
        "id": "EgUObdBWOiuA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lITXUOKDOnW6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}